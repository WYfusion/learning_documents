# 参数的优化器

目的是解决将大量数据进行分配形成多个批次引发的各个批次的迭代方向不是向最优的方向，而是曲折的甚至有可能陷入局部最优解。

#### 1、$SGD$优化器$(Stochastic\ Gradient\ Descent)$

$$
w_{t+1}=w_t-\alpha\cdot g(w_t)
$$

$g(w_t)$为$t$时刻对参数$w_t$的梯度损失

##### 缺点：

1. 易受样本噪声影响
2. 可能陷入局部最优解

#### 2、$SGD+Momentum$优化器

$$
\begin{aligned}&V_t=\eta·V_{t-1}+\alpha\cdot g(w_t)\\&w_{t+1}=w_t-V_t\end{aligned}
$$

$\eta$为动量系数，一般是取$\eta$为$0.9$ 。

#### 3、$Adagrad$优化器(自适应学习率)

$$
\begin{aligned}&s_t=s_{t-1}+g(w_t)\cdot g(w_t)\\&w_{t+1}=w_t-\frac\alpha{\sqrt{s_t+\varepsilon}}\cdot g(w_t)\end{aligned}
$$

$\varepsilon$ 是$10^{-7}$作用是避免分母为0。

##### 缺点：学习率下降的太快，可能还未收敛就停止训练了

#### 4、 $RMSProp$优化器(自适应学习率)

$$
\begin{aligned}&s_t=\eta\cdot s_{t-1}+(1-\eta)\cdot g(w_t)\cdot g(w_t)\\&w_{t+1}=w_t-\frac\alpha{\sqrt{s_t+\varepsilon}}\cdot g(w_t)\end{aligned}
$$

#### 5、$Adam$优化器(自适应学习率)

$$
\begin{aligned}
&m_t=\beta_1\cdot m_{t-1}+(1-\beta_1)\cdot g(w_t)\quad\text{一阶动量} \\
&\nu_t=\beta_2\cdot\nu_{t-1}+(1-\beta_2)\cdot g(w_t)\cdot g(w_t)\quad\text{二阶动量} \\
&\hat{m}_t=\frac{m_t}{1-\beta_1^t}\quad\hat{\nu}_t=\frac{\nu_t}{1-\beta_2^t} \\
&w_{t+1}=w_t-\frac{\alpha}{\sqrt{\hat{\nu}_t+\varepsilon}}\hat{m}_t
\end{aligned}
$$

$\beta_1$与$\beta_2$控制衰减速度的通常取$0.9$和$0.999$